# Fazy Stabilizacji MVP - Dokumentacja

## Faza 1: Stabilizacja MVP ‚úÖ

### Ujednolicenie eksportu JSON/HTML

**Utworzone:**
- `utils/export_utils.py` - Wsp√≥lny modu≈Ç eksportu dla GUI i CLI
  - `export_json()` - Ujednolicony eksport JSON
  - `export_html()` - Ujednolicony eksport HTML z ≈Çadnym stylingiem
  - `generate_html_report()` - Generowanie raportu HTML

**Zaktualizowane:**
- `gui_mvp.py` - U≈ºywa wsp√≥lnego modu≈Çu eksportu
- `main.py` - U≈ºywa wsp√≥lnego modu≈Çu eksportu (JSON + HTML)

**Rezultat:**
- ‚úÖ GUI i CLI u≈ºywajƒÖ tego samego formatu eksportu
- ‚úÖ Sp√≥jny format JSON/HTML w ca≈Çej aplikacji
- ‚úÖ ≈Åadny styling HTML z responsywnym designem

### Sprawdzenie GUI MVP

**Status:**
- ‚úÖ GUI MVP wy≈õwietla pe≈Çne dane z wszystkich collector√≥w
- ‚úÖ Lista collector√≥w z statusami dzia≈Ça poprawnie
- ‚úÖ PodglƒÖd surowych danych dzia≈Ça
- ‚úÖ Eksport JSON/HTML dzia≈Ça

## Faza 2: Pipeline i asynchroniczno≈õƒá ‚úÖ

### Timeouty i fallback dla collector√≥w

**Utworzone:**
- `collectors/collector_master_with_timeouts.py` - Wersja z timeoutami
  - `collect_all_with_timeouts()` - Async z timeoutami
  - `_run_collector_with_timeout()` - Uruchamianie z timeoutem
  - Obs≈Çuga `asyncio.TimeoutError`

**Funkcjonalno≈õci:**
- ‚úÖ Timeouty konfigurowalne przez `config.json` ‚Üí `collectors.timeout_seconds`
- ‚úÖ Fallback - b≈Çƒôdy/timeouty nie przerywajƒÖ innych collector√≥w
- ‚úÖ Licznik timeout√≥w w summary

### Centralny Logger

**Status:**
- ‚úÖ Logger rejestruje statusy wszystkich collector√≥w (`log_collector_start/end`)
- ‚úÖ Logger rejestruje statusy wszystkich procesor√≥w (`log_processor_start/end`)
- ‚úÖ Logger rejestruje metryki wydajno≈õci (`log_performance`)

**Zaktualizowane:**
- `collectors/collector_master.py` - Rejestruje w monitorze wydajno≈õci
- `collectors/collector_master_async.py` - Rejestruje w monitorze wydajno≈õci
- `processors/analyzer.py` - Rejestruje w monitorze wydajno≈õci

### Monitorowanie wydajno≈õci

**Utworzone:**
- `utils/performance_monitor.py` - Monitor wydajno≈õci
  - `PerformanceMonitor` - Klasa monitorujƒÖca metryki
  - `record_collector()` - Rejestruje metryki collectora
  - `record_processor()` - Rejestruje metryki procesora
  - `get_all_stats()` - Zwraca wszystkie statystyki
  - `log_summary()` - Loguje podsumowanie wydajno≈õci

**Funkcjonalno≈õci:**
- ‚úÖ Zbieranie metryk dla ka≈ºdego collectora (czas wykonania, status, liczba danych)
- ‚úÖ Zbieranie metryk dla ka≈ºdego procesora (czas wykonania, b≈Çƒôdy, ostrze≈ºenia)
- ‚úÖ Statystyki (≈õredni czas, min, max, success rate)
- ‚úÖ Top 5 najwolniejszych collector√≥w
- ‚úÖ Podsumowanie wydajno≈õci w CLI

## Faza 3: Testy i CI/CD ‚úÖ

### Coverage test√≥w dla async pipeline

**Utworzone:**
- `tests/test_async_pipeline.py` - Testy asynchronicznego pipeline
  - `TestAsyncPipeline` - Testy podstawowe async
  - `TestAsyncCollectors` - Testy z pytest-asyncio
  - Testy timeout√≥w
  - Testy wydajno≈õci async vs sync

**Status:**
- ‚úÖ Testy pokrywajƒÖ async pipeline
- ‚úÖ Testy timeout√≥w
- ‚úÖ Testy wydajno≈õci

### CI/CD (GitHub Actions)

**Utworzone:**
- `.github/workflows/ci.yml` - GitHub Actions workflow
  - Uruchamianie test√≥w na Windows
  - Generowanie raportu coverage
  - Upload coverage do Codecov
  - Walidacja diagram√≥w PlantUML
  - Sprawdzanie jako≈õci kodu (flake8)

**Funkcjonalno≈õci:**
- ‚úÖ Automatyczne uruchamianie test√≥w przy push/PR
- ‚úÖ Generowanie raportu coverage (HTML + XML)
- ‚úÖ Walidacja diagram√≥w PlantUML
- ‚úÖ Sprawdzanie jako≈õci kodu

## Faza 4: Przygotowanie na skalowalno≈õƒá ‚úÖ

### Benchmarki wydajno≈õci

**Utworzone:**
- `tests/benchmark_collectors.py` - Benchmarki wydajno≈õci
  - `test_async_vs_sync_performance()` - Por√≥wnanie async vs sync
  - `test_timeout_performance()` - Test wydajno≈õci z timeoutami
  - `test_large_scale_collectors()` - Test skalowalno≈õci

**Funkcjonalno≈õci:**
- ‚úÖ Benchmarki por√≥wnujƒÖce async vs sync
- ‚úÖ Testy wydajno≈õci przy du≈ºej liczbie collector√≥w
- ‚úÖ Metryki czasu wykonania

### Ograniczenie r√≥wnoleg≈Ço≈õci (asyncio.Semaphore)

**Zaimplementowane:**
- `collectors/collector_master_async.py` - Dodano semafor
  - `max_concurrent` w config.json
  - `asyncio.Semaphore` dla ograniczenia r√≥wnoleg≈Ço≈õci
  - Automatyczne u≈ºycie przy du≈ºych skanach

**Konfiguracja:**
```json
{
  "collectors": {
    "max_concurrent": null  // null = bez limitu, liczba = limit r√≥wnoleg≈Ço≈õci
  }
}
```

### Rozszerzony dashboard (przygotowanie)

**Status:**
- ‚è≥ Przygotowanie - wymaga dodatkowych modu≈Ç√≥w
- ‚úÖ Monitor wydajno≈õci gotowy do u≈ºycia w dashboardzie
- ‚úÖ Statystyki historyczne mo≈ºna agregowaƒá z log√≥w

## U≈ºycie

### Eksport raport√≥w (Faza 1)

**GUI:**
- Kliknij "üíæ Export JSON" lub "üìÑ Export HTML"
- Wybierz lokalizacjƒô pliku

**CLI:**
- Automatyczny eksport JSON i HTML po skanie
- Pliki w `output/processed/`

### Timeouty (Faza 2)

**Konfiguracja:**
```json
{
  "collectors": {
    "timeout_seconds": 300  // 5 minut
  }
}
```

**U≈ºycie:**
```python
from collectors.collector_master_with_timeouts import collect_all_with_timeouts_wrapper
result = collect_all_with_timeouts_wrapper(timeout_seconds=60)
```

### Monitorowanie wydajno≈õci (Faza 2)

```python
from utils.performance_monitor import get_performance_monitor

monitor = get_performance_monitor()
stats = monitor.get_all_stats()
monitor.log_summary()
```

### Ograniczenie r√≥wnoleg≈Ço≈õci (Faza 4)

**Konfiguracja:**
```json
{
  "collectors": {
    "max_concurrent": 6  // Maksymalnie 6 collector√≥w r√≥wnocze≈õnie
  }
}
```

## Przysz≈Çe ulepszenia

- [ ] Dashboard webowy z metrykami historycznymi
- [ ] Agregacja danych z wielu skan√≥w
- [ ] Wykresy wydajno≈õci w czasie
- [ ] Alerty przy spadku wydajno≈õci
- [ ] Eksport metryk do Prometheus/Grafana

